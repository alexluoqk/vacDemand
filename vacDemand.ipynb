{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow\n","from matplotlib import pyplot as plt\n","import datetime as dt\n","from pandas import concat as cot\n","from math import sqrt\n","from numpy import concatenate\n","from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Reshape\n","from tensorflow.keras.layers import LSTM\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n","from sklearn.metrics import r2_score\n","from tensorflow.keras import layers, Model, utils\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n","from numpy import median\n","from numpy import mean\n","import chinese_calendar\n","\n","data = pd.read_csv('../../data/originalDataRegion.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["data['Date'] = pd.date_range('2020-12-15', '2022-01-16')\n","start_time = dt.date(2020, 12, 15)\n","end_time = dt.date(2022, 1, 16)\n","\n","# Holiday feature boolean(add)\n","holiday_list = chinese_calendar.get_holidays(start_time, end_time)\n","data['is_holiday'] = np.where(data['Date'].isin(holiday_list), 1, 0)\n","\n","# IssuePoint feature boolean\n","data['V1_IssuePoint'] = np.where(data['V1_IssuePoint'].notna(), 1, 0)\n","data['V2_H7_IssuePoint'] = np.where(data['V2_H7_IssuePoint'].notna(), 1, 0)\n","data['V3_IssuePoint'] = np.where(data['V3_IssuePoint'].notna(), 1, 0)\n","data['VacCategory'] = np.where(data['VacCategory'].notna(), 1, 0)\n","\n","data['diff_VacDailySmoothed'] = data['VacDailySmoothed'].diff(1)\n","\n","for i in [7, 14]:\n","\tw = int(i/7)\n","\tdata['{}w_lagVacDailySmoothed'.format(w)] = 0\n","\tfor j in range(1, i+1):\n","\t\tdata['{}w_lagVacDailySmoothed'.format(w)] += data['VacDailySmoothed'].shift(j)\n","\n","data['deltaVacWeekly'] = data['1w_lagVacDailySmoothed'].diff(periods=7)\n","deltaVacWeekly = np.array(data['deltaVacWeekly'])\n","where_are_inf = np.isinf(deltaVacWeekly)\n","where_are_nan = np.isnan(deltaVacWeekly)\n","deltaVacWeekly[where_are_inf] = 1\n","deltaVacWeekly[where_are_nan] = 0\n","\n","data['deltaVacWeekly'] = deltaVacWeekly\n","\n","def exponential_decay(t, init=0.5, m=6, finish=0.4):\n","    alpha = np.log(init / finish) / m\n","    l = - np.log(init) / alpha\n","    decay = np.exp(-alpha * (t + l))\n","    return decay\n","for i in data.columns[1:40].values:\n","\tdata[i] = \\\n","\t\t(data[i].shift(1) * exponential_decay(t=0) + \\\n","\t\tdata[i].shift(2) * exponential_decay(t=1) + \\\n","\t\tdata[i].shift(3) * exponential_decay(t=2) + \\\n","\t\tdata[i].shift(4) * exponential_decay(t=3) + \\\n","\t\tdata[i].shift(5) * exponential_decay(t=4) + \\\n","\t\tdata[i].shift(6) * exponential_decay(t=5) + \\\n","\t\tdata[i].shift(7) * exponential_decay(t=6))/7\n","\n","data.set_index('Date', inplace=True)\n","data = data.drop(columns='index')\n","#data = data.fillna(0)\n","\n","pdata = data.copy()\n","pdata = pdata.drop(columns=['new_deaths', 'new_deaths_smoothed', 'weekly_cases', 'biweekly_cases', 'daily_ people_1_dose',     \t\t\t\t\t\t\t'total_vaccinations', 'people_1_dose', 'people_2_doses', 'people_boosters', 'people_vaccinated_per_hundred',\n","\t\t\t\t\t'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred'])\n","pdata = pdata.fillna(0)\n","\n","pdata['mean_7d_reproduction_rate'] = (pdata['reproduction_rate'].shift(1) + \\\n","\t\t\t\t\t\t\t\t\t  pdata['reproduction_rate'].shift(2) + \\\n","\t\t\t\t\t\t\t\t\t  pdata['reproduction_rate'].shift(3) + \\\n","\t\t\t\t\t\t\t\t\t  pdata['reproduction_rate'].shift(4) + \\\n","\t\t\t\t\t\t\t\t\t  pdata['reproduction_rate'].shift(5) + \\\n","\t\t\t\t\t\t\t\t\t  pdata['reproduction_rate'].shift(6) + \\\n","\t\t\t\t\t\t\t\t\t  pdata['reproduction_rate'].shift(7))/7\n","\n","pdata['is_sensitive'] = np.where(pdata['mean_7d_reproduction_rate'] <= 1.0, 0, 1)\n","\n","pdata['vac_aggressive'] = np.where((((pdata['is_sensitive'] == 1)&(pdata['new_cases'].shift(1) >= 30)&(pdata['new_cases'].shift(1) <= 300)) | ((pdata['is_sensitive'] == 0)&(pdata['new_cases'].shift(1) >= 60)&(pdata['new_cases'].shift(1) <= 300))), 1, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def series_to_supervised(data, n_in=1, span=-1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = pd.DataFrame(data)\n","\tcols, names = list(), list()\n","\n","\tfor i in range(n_in, 0, -span):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\n","\tfor i in range(0, n_out, span):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\n","\tagg = cot(cols, axis=1)\n","\tagg.columns = names\n","\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg\n","\n","daily_scaler = MinMaxScaler(feature_range=(0, 1))\n","delta_scaler = MaxAbsScaler()\n","\n","train_scaler = MaxAbsScaler()\n","pdata['VacDailySmoothed'] = daily_scaler.fit_transform(pdata[['VacDailySmoothed']])\n","pdata['deltaVacWeekly'] = delta_scaler.fit_transform(pdata[['deltaVacWeekly']])\n","\n","\n","scaled_to_trans = pdata.iloc[:, np.r_[1, 8, 9, 40:45, 50, 57]]\n","\n","temp_col = scaled_to_trans.VacDailySmoothed\n","scaled_to_trans = scaled_to_trans.drop('VacDailySmoothed',axis=1)\n","scaled_to_trans.insert(0,'VacDailySmoothed', temp_col)\n","\n","scaled = train_scaler.fit_transform(scaled_to_trans)\n","\n","scaled = scaled.astype('float32')\n","\n","n_days = 7\n","span = 1\n","n_after = 1\n","n_features = 10\n","input_sample = int(n_days/span)\n","\n","reframed = series_to_supervised(scaled, n_days, span, n_after)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(224, 70) 224 (224,)\n","(224, 7, 10) (224,) (63, 7, 10) (63,)\n"]}],"source":["ts_train_1 = pdata.truncate(before='2020-12-15', after='2021-04-11')\n","ts_train_2 = pdata.truncate(before='2020-12-15', after='2021-07-01')\n","ts_train_3 = pdata.truncate(before='2020-12-15', after='2022-12-08')\n","\n","ts_1 = pdata.truncate(before='2020-12-15', after='2021-04-25')\n","ts_2 = pdata.truncate(before='2020-12-15', after='2021-07-15')\n","ts_3 = pdata.truncate(before='2020-12-15', after='2021-12-22')\n","\n","ts_test_1 = pdata.truncate(before='2021-04-18', after='2021-04-25')\n","ts_test_2 = pdata.truncate(before='2021-07-08', after='2021-07-15')\n","ts_test_3 = pdata.truncate(before='2021-12-15', after='2021-12-22')\n","\n","t1 = pdata.truncate(before='2020-12-15', after='2021-02-01')\n","t2 = pdata.truncate(before='2020-12-15', after='2021-06-15')\n","t3 = pdata.truncate(before='2020-12-15', after='2021-06-15')\n","t4 = pdata.truncate(before='2020-12-15', after='2021-08-15')\n","\n","values = reframed.values\n","train_scene = values[len(t1):len(ts_2)+60, :]\n","test_scene = values[len(ts_3)-45:, :]\n","\n","n_obs = input_sample * n_features\n","#s2 as train and s3 as test\n","train_X, train_y = train_scene[:, :n_obs], train_scene[:, -n_features]\n","test_X, test_y = test_scene[:, :n_obs], test_scene[:, -n_features]\n","\n","print(train_X.shape, len(train_X), train_y.shape)\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], input_sample, n_features))\n","test_X = test_X.reshape((test_X.shape[0], input_sample, n_features))\n","print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#training\n","\n","def mape(y_true, y_pred):\n","    mask=y_true!=0\n","    return np.fabs((y_true[mask]-y_pred[mask])/np.clip(y_true[mask],0.1,1)).mean()\n","\n","def smape(y_true, y_pred):\n","    return 2.0 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n","\n","log_dir=\"logs/fit/\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","model_dir=\"my_model/train/\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tbCallBack = TensorBoard(log_dir=log_dir,update_freq='epoch', histogram_freq=1,write_graph=True, write_images=True)\n","earlyStop = EarlyStopping(monitor=\"val_loss\", verbose=2, mode='min', patience=3)\n","callbacks = [tbCallBack]\n","repeats = 20\n","error_scores, adjusted_r2scores, mse_scores, mae_scores, mape_scores, smape_scores, bestModelList = list(), list(), list(), list(), list(), list(), list()\n","\n","inv_yhat_mean = np.array(pd.DataFrame(columns=range(len(test_y))))\n","inv_yhat_mean = 0\n","batch_size = 16\n","epochs = 100\n","\n","for r in range(repeats):\n","    \n","    model = Sequential()\n","\n","    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n","\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1))\n","    \n","    model.compile(loss='mse', optimizer='adam', metrics= ['accuracy'])\n","\n","    print('train...')\n","\n","    model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=1, shuffle=False, callbacks=callbacks)\n","\n","    print('predict...')\n","    yhat = model.predict(test_X)\n","    test_X_p = test_X.reshape((test_X.shape[0], input_sample * n_features))\n","\n","    inv_yhat = concatenate((yhat, test_X_p[:, -9:]), axis=1)\n","    inv_yhat = train_scaler.inverse_transform(inv_yhat)\n","    inv_yhat = inv_yhat[:,0]\n","\n","    test_y_p = test_y.reshape((len(test_y), 1))\n","    inv_y = concatenate((test_y_p, test_X_p[:, -9:]), axis=1)\n","    inv_y = train_scaler.inverse_transform(inv_y)\n","    inv_y = inv_y[:,0]\n","\n","    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n","    mse = mean_squared_error(inv_y, inv_yhat)\n","    mae = mean_absolute_error(inv_y, inv_yhat)\n","    mape_value = mape(inv_y, inv_yhat)\n","    smape_value = smape(inv_y, inv_yhat)\n","\n","    if rmse < 0.05:\n","        inv_yhat_mean += np.array(inv_yhat)\n","        bestModelList.append(r+1)\n","        model.save(\"../../results/\" + model_dir + \"/%d_%d_rmse%.3f\" % (r+1, repeats, rmse))\n","\n","    n = (train_X.shape[0] + test_X.shape[0])\n","    Adjusted_R2 = 1-(((1-r2_score(inv_y,inv_yhat))*(n-1))/(n-n_features-1))\n","    print('%d) Test RMSE: %.3f' % (r+1, rmse))\n","    print('%d) Test Adjusted_R2: %.3f' % (r+1, Adjusted_R2))\n","    # MAE\n","    print('%d) Test MAE: %.3f' % (r+1, mae))\n","    # MSE\n","    print('%d) Test MSE: %.3f' % (r+1, mse))\n","    # MAPE\n","    print('%d) Test MAPE: %.3f' % (r+1, mape_value))\n","    # SMAPE\n","    print('%d) Test SMAPE: %.3f' % (r+1, smape_value))\n","\n","    error_scores.append(rmse)\n","    adjusted_r2scores.append(Adjusted_R2)\n","    mae_scores.append(mae)\n","    mse_scores.append(mse)\n","    mape_scores.append(mape_value)\n","    smape_scores.append(smape_value)\n","\n","results = pd.DataFrame()\n","results['error_scores'] = error_scores\n","results['adjusted_r2scores'] = adjusted_r2scores\n","results['mae_scores'] = mae_scores\n","results['mse_scores'] = mse_scores\n","results['mape_value'] = mape_scores\n","results['smape_value'] = smape_scores\n","print(results.describe())\n","results.to_csv('../../results/experiment_stateful.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inv_yhat_mean_display = inv_yhat_mean / len(bestModelList)\n","results = pd.read_csv('../../results/experiment_stateful.csv', header=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["relative_error = 0.\n","for i in range(len(test_y)):\n","    relative_error += (abs(inv_yhat_mean_display[i] - test_y[i]) / test_y[i]) ** 2\n","acc = 1- sqrt(relative_error / len(test_y))\n","fh = open('../../results/accuracy.txt', 'w', encoding='utf-8')\n","fh.write(f'Accuracy of model: {acc*100:.2f}%')\n","fh.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from lime import lime_tabular\n","\n","explainer = lime_tabular.RecurrentTabularExplainer(train_X,\n","                                                feature_names=['VacDailySmoothed', 'V1_5-15', 'V1_Clinical', 'V1_Crowded', 'V2_H7_IssuePoint', 'V3_IssuePoint', 'VacCategory','CHI', 'is_holiday', 'vac_aggressive'], \n","                                                verbose=True, \n","                                                mode='regression')\n","i = 40\n","exp = explainer.explain_instance(test_X[i], model.predict, num_features=10)\n","import json\n","import os\n","def write_list_to_json(list, json_file_name, json_file_save_path):\n","    os.chdir(json_file_save_path)\n","    with open(json_file_name, 'w') as  f:\n","        json.dump(list, f)\n","\n","write_list_to_json(exp.as_list(), 'lime_results.json', '../../results/')"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":2}
